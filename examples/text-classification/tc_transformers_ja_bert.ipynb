{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Transformers BERT モデル (PyTorch) による日本語文章のテキスト分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapbook as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# # path の追加\n",
    "# sys.path.append('../..')\n",
    "\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.common.pytorch_utils import dataloader_from_dataset\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.models.transformers.sequence_classification import (\n",
    "    Processor, SequenceClassifier)\n",
    "\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "# 表示する列データの幅を変更\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 念のため Transformer Version 確認\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 モデルの設定\n",
    "本ノートブックでは日本語対応BERTモデルのファインチューニングと評価を行います。\n",
    "\n",
    "ここでは、[Hugging Face's PyTorch implementation](https://github.com/huggingface/transformers) をラップした [sequence classifier](../../utils_nlp/models/transformers/sequence_classification.py)を利用します。本コードでは、**bert-base-japanase-whole-word-masking** という学習済みモデルを利用します。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "DATA_FOLDER = TemporaryDirectory().name\n",
    "CACHE_DIR = TemporaryDirectory().name\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "NUM_GPUS = 2\n",
    "MAX_LEN = 100\n",
    "TRAIN_DATA_FRACTION =1 #サンプリングする場合は割合(<1)を指定\n",
    "TEST_DATA_FRACTION =1 #サンプリングする場合は割合(<1)を指定\n",
    "TRAIN_SIZE = 0.75\n",
    "LABEL_COL = \"label\" # ラベルを含む列名\n",
    "TEXT_COL = \"text\" # テキストを含む列名\n",
    "MODEL_NAMES = [\"bert-base-japanese-whole-word-masking\"] #利用するモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 データ準備\n",
    "#### ダウンロード\n",
    "[Livedoor ニュースコーパス](https://www.rondhuit.com/download/ldcc-20140209.tar.gz)をダウンロードして利用します。\n",
    "<!-- データのダウンロードと加工手順は [bert-japanese](https://github.com/yoheikikuta/bert-japanese/) を参考にしています。 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlretrieve\n",
    "# import tarfile\n",
    "\n",
    "# text_url = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
    "# file_path = \"./ldcc-20140209.tar.gz\"\n",
    "# urlretrieve(text_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gz ファイルを解凍します。\n",
    "# with tarfile.open('./ldcc-20140209.tar.gz', 'r:gz') as tar:\n",
    "#     tar.extractall(path='livedoor')\n",
    "#     tar.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas へのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['url', 'date', 'label', 'title', 'text']\n",
    "df = pd.DataFrame(columns = columns)\n",
    "#df.set_index('url',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"livedoor/text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name in os.listdir(path):\n",
    "    print(folder_name)\n",
    "    if folder_name.endswith(\".txt\") :\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join(path, folder_name)):\n",
    "        if folder_name == \"LICENSE.txt\" :\n",
    "            continue\n",
    "        with open(os.path.join(path, folder_name, file), 'r') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            if len(lines) == 1:\n",
    "                continue\n",
    "            url = lines[0]\n",
    "            date = lines[1]\n",
    "            label = folder_name\n",
    "            title = lines[3]\n",
    "            text = \"\".join(lines[4:])\n",
    "            data = {'url': url, 'date':date, 'label': label, 'title':title, 'text':text}\n",
    "        s = pd.Series(data)        \n",
    "        df = df.append(s, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"livedoor-corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Azure Machine Learning ワークスペース接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #テナントIDを指定する方法\n",
    "# from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "# interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "# ws = Workspace.from_config(auth=interactive_auth)\n",
    "# print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Azure Machine Learning データセット登録と準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore.upload_files(files=['livedoor-corpus.csv'],\n",
    "                       target_path='livedoor-corpus',\n",
    "                       overwrite=True,\n",
    "                       show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_path = [(datastore, 'livedoor-corpus/livedoor-corpus.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedoor_ds = Dataset.Tabular.from_delimited_files(path=datastore_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedoor_ds.register(workspace=ws, name='livedoor',description='livedoor corpus', create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws, name='livedoor')\n",
    "df = dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値を除外します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[pd.isna(df[\"text\"])==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本データセットでは9種類のラベルに分類されます。それぞれのデータ数を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(df[LABEL_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを学習用、テスト用に分割します。またラベル名をエンコーディングしてBERTで扱えるようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ分割\n",
    "df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 必要であればサンプリング\n",
    "# df_train = df_train.sample(frac=TRAIN_DATA_FRACTION).reset_index(drop=True)\n",
    "# df_test = df_test.sample(frac=TEST_DATA_FRACTION).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルのエンコーディング\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[LABEL_COL] = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "df_test[LABEL_COL] = label_encoder.transform(df_test[LABEL_COL])\n",
    "\n",
    "num_labels = len(np.unique(df_train[LABEL_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. モデル学習と評価\n",
    "### 2.1 学習済みモデルの選択\n",
    "\n",
    "[Hugging Face](https://github.com/huggingface/transformers) には学習済みモデルが公開されており簡単に利用することができます。テキスト分類で利用できるモデル一覧を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"モデル名\": SequenceClassifier.list_supported_models()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ファインチューニング\n",
    "\n",
    "本コードで実装されているラッパーを利用することで簡単にファインチューニングが実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用するモデル名\n",
    "print(MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ前処理、ファインチューニング、テストデータの予測、評価をステップを実施していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "model_name = MODEL_NAMES[0]\n",
    "\n",
    "# 前処理\n",
    "processor = Processor(\n",
    "    model_name=model_name,\n",
    "    to_lower=model_name.endswith(\"uncased\"),\n",
    "    cache_dir=CACHE_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = processor.dataset_from_dataframe(\n",
    "    df_train, TEXT_COL, LABEL_COL, max_len=MAX_LEN\n",
    ")\n",
    "train_dataloader = dataloader_from_dataset(\n",
    "    train_dataset, batch_size=BATCH_SIZE, num_gpus=NUM_GPUS, shuffle=True\n",
    ")\n",
    "test_dataset = processor.dataset_from_dataframe(\n",
    "    df_test, TEXT_COL, LABEL_COL, max_len=MAX_LEN\n",
    ")\n",
    "test_dataloader = dataloader_from_dataset(\n",
    "    test_dataset, batch_size=BATCH_SIZE, num_gpus=NUM_GPUS, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニング\n",
    "classifier = SequenceClassifier(\n",
    "    model_name=model_name, num_labels=num_labels, cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(\n",
    "        train_dataloader, num_epochs=NUM_EPOCHS, num_gpus=NUM_GPUS, verbose=False,\n",
    "    )\n",
    "train_time = t.interval / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの予測\n",
    "preds = classifier.predict(test_dataloader, num_gpus=NUM_GPUS, verbose=False)\n",
    "\n",
    "# 評価\n",
    "accuracy = accuracy_score(df_test[LABEL_COL], preds)\n",
    "class_report = classification_report(\n",
    "    df_test[LABEL_COL], preds, target_names=label_encoder.classes_, output_dict=True\n",
    ")\n",
    "\n",
    "# 結果の保存\n",
    "results[model_name] = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"f1-score\": class_report[\"macro avg\"][\"f1-score\"],\n",
    "    \"time(hrs)\": train_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[LABEL_COL] = label_encoder.inverse_transform(df_test[LABEL_COL])\n",
    "df_test[\"pred\"] = label_encoder.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 評価\n",
    "\n",
    "精度、F1-スコア、学習時間を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "sb.glue(\"accuracy\", df_results.iloc[0, :].mean())\n",
    "sb.glue(\"f1\", df_results.iloc[1, :].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
